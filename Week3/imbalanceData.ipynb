{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e7e1a2",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data for Classification\n",
    "\n",
    "A key component of machine learning classification tasks is handling unbalanced data, which is characterized by a skewed class distribution with a considerable overrepresentation of one class over the others. The difficulty posed by this imbalance is that models may exhibit inferior performance due to bias towards the majority class. When faced with uneven settings, the model's bias is to value accuracy over accurately recognizing occurrences of minority classes.\n",
    "\n",
    "This problem can be solved by applying specialized strategies like resampling (oversampling minority class, undersampling majority class), utilizing various assessment measures (F1-score, precision, recall), and putting advanced algorithms to work with unbalanced datasets into practice.\n",
    "\n",
    "**What is Imbalanced Data and How to handle it?**\n",
    "Imbalanced data pertains to datasets where the distribution of observations in the target class is uneven. In other words, one class label has a significantly higher number of observations, while the other has a notably lower count.\n",
    "\n",
    "When one class greatly outnumbers the others in a classification, there is imbalanced data. Machine learning models may become biased in their predictions as a result, favoring the majority class. Techniques like oversampling the minority class or undersampling the majority class are used in resampling to remedy this.\n",
    "\n",
    "Furthermore, it is possible to evaluate model performance more precisely by substituting other assessment measures, such as precision, recall, or F1-score, for accuracy. To further improve the handling of imbalanced datasets for more reliable and equitable predictions, specialized techniques such as ensemble approaches and the incorporation of synthetic data generation can be used.\n",
    "\n",
    "**Problem with Handling Imbalanced Data for Classification**\n",
    "- Algorithms may get biased towards the majority class and thus tend to predict output as the majority class.\n",
    "- Minority class observations look like noise to the model and are ignored by the model.\n",
    "- Imbalanced dataset gives misleading accuracy score.\n",
    "\n",
    "**Ways to handle Imbalanced Data for Classification**\n",
    "Addressing imbalanced data in classification is crucial for fair model performance. Techniques include resampling (oversampling or undersampling), synthetic data generation, specialized algorithms, and alternative evaluation metrics. Implementing these strategies ensures more accurate and unbiased predictions across all classes.\n",
    "\n",
    "1. Different Evaluation Metric\n",
    "Classifier accuracy is calculated by dividing the total correct predictions by the overall predictions, suitable for balanced classes but less effective for imbalanced datasets. Precision gauges the accuracy of a classifier in predicting a specific class, while recall assesses its ability to correctly identify a class. In imbalanced datasets, the F1 score emerges as a preferred metric, striking a balance between precision and recall, providing a more comprehensive evaluation of a classifier's performance. It can be expressed as the mean of recall and accuracy.\n",
    "\n",
    "F1= 2 * precision+recall over precision∗recall\n",
    "​\n",
    "Precision and F1 score both decrease when the classifier incorrectly predict the minority class, increasing the number of false positives. Recall and F1 score also drop if the classifier have trouble accurately identifying the minority class, leading to more false negatives. In particular, the F1 score only becomes better when the amount and accuracy of predictions get better.\n",
    "\n",
    "F1 score is essentially a comprehensive statistic that takes into account the trade-off between precision and recall, which is critical for assessing classifier performance in datasets that are imbalanced.\n",
    "\n",
    "2. Resampling (Undersampling and Oversampling)\n",
    "This method involves adjusting the balance between minority and majority classes through upsampling or downsampling. In the case of an imbalanced dataset, oversampling the minority class with replacement, termed oversampling, is employed. Conversely, undersampling entails randomly removing rows from the majority class to align with the minority class.\n",
    "\n",
    "This sampling approach yields a balanced dataset, ensuring comparable representation for both majority and minority classes. Achieving a similar number of records for both classes in the dataset signifies that the classifier will assign equal importance to each class during training.\n",
    "\n",
    "**Observation**\n",
    "- make_classification creates an imbalanced dataset with 10% minority class (class 0) and 90% majority class (class 1)\n",
    "- Original distribution shows approximately 100 samples of class 0 and 900 samples of class 1\n",
    "- RandomOverSampler duplicates minority class samples to match majority class count (both reach ~900 samples)\n",
    "- RandomUnderSampler removes majority class samples to match minority class count (both reach ~100 samples)\n",
    "- Both resampling techniques achieve balanced class distribution for fair model training\n",
    "\n",
    "**Findings**\n",
    "- Oversampling increases minority class representation by randomly duplicating existing samples with replacement\n",
    "- Undersampling decreases majority class representation by randomly removing samples without replacement\n",
    "- Balanced datasets prevent models from becoming biased toward the majority class\n",
    "- Oversampling risks overfitting due to duplicate data; undersampling risks losing valuable majority class information\n",
    "- Both techniques enable classifiers to assign equal importance to each class during training, improving minority class prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70e8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({np.int64(1): 900, np.int64(0): 100})\n",
      "Oversampled class distribution: Counter({np.int64(1): 900, np.int64(0): 900})\n",
      "Undersampled class distribution: Counter({np.int64(0): 100, np.int64(1): 100})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "\n",
    "# Oversampling using RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "print(\"Oversampled class distribution:\", Counter(y_over))\n",
    "\n",
    "\n",
    "# Undersampling using RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "print(\"Undersampled class distribution:\", Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0468d",
   "metadata": {},
   "source": [
    "3. BalancedBaggingClassifier\n",
    "When dealing with imbalanced datasets, traditional classifiers tend to favor the majority class, neglecting the minority class due to its lower representation. The BalancedBaggingClassifier, an extension of sklearn classifiers, addresses this imbalance by incorporating additional balancing during training. It introduces parameters like \"sampling_strategy,\" determining the type of resampling (e.g., 'majority' for resampling only the majority class, 'all' for resampling all classes), and \"replacement,\" dictating whether the sampling should occur with or without replacement. This classifier ensures a more equitable treatment of classes, particularly beneficial when handling imbalanced datasets.\n",
    "\n",
    "***Importing Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa6412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0c8e5",
   "metadata": {},
   "source": [
    "This code demonstrates the usage of a BalancedBaggingClassifier from the imbalanced-learn library to handle imbalanced datasets. It creates an imbalanced dataset, splits it, and trains a Random Forest classifier with balanced bagging, assessing the model's performance through accuracy and a classification report.\n",
    "\n",
    "***Creating imbalanced dataset and splitting***\n",
    "\n",
    "\n",
    "**Observation**\n",
    "- Creates 1000 samples with 20 features\n",
    "- Class distribution: 10% minority class (100 samples) and 90% majority class (900 samples)\n",
    "- Training set gets 80% of data (800 samples)\n",
    "- Test set gets 20% of data (200 samples)\n",
    "\n",
    "**Findings**\n",
    "- The imbalanced dataset simulates real-world problems where one class is rare\n",
    "- Training data: ~80 minority + ~720 majority class samples\n",
    "- Test data: ~20 minority + ~180 majority class samples\n",
    "- Splitting first prevents data leakage—test data stays completely separate\n",
    "- The dataset is now ready for resampling techniques or special classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067f4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e9156",
   "metadata": {},
   "source": [
    "This code creates two-class, imbalanced datasets, divides them into training and testing sets, and uses a predetermined random state to guarantee reproducibility. With 20 features in the final dataset, the minority class has a weight of 0.1, indicating a notable class imbalance.\n",
    "\n",
    "***Creating a random forest classifier***\n",
    "\n",
    "**Observation**\n",
    "- RandomForestClassifier is initialized with random_state=42 for reproducibility\n",
    "- This creates a base classifier that will be used with BalancedBaggingClassifier\n",
    "- The classifier is not trained yet—it's just instantiated\n",
    "- random_state=42 ensures the same results every time you run the code\n",
    "\n",
    "**Findings**\n",
    "- Random Forest uses multiple decision trees to make predictions\n",
    "- It's a good base classifier for handling imbalanced data because trees naturally handle class imbalance better than linear models\n",
    "- The classifier is ready to be wrapped in BalancedBaggingClassifier for additional balancing\n",
    "- Random state ensures reproducible results across different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80cf9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier (you can use any classifier)\n",
    "base_classifier = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492d60c",
   "metadata": {},
   "source": [
    "By initializing a Random Forest classifier with a given random state, this method creates a base classifier that may be used in subsequent analyses. Reproducibility in model training is guaranteed by the random state.\n",
    "\n",
    "***Creating a balanced bagging classifier***\n",
    "\n",
    "**Observation**\n",
    "- Wraps the base classifier with balanced bagging to handle class imbalance\n",
    "- sampling_strategy='auto' balances classes automatically\n",
    "- replacement=False samples without replacement\n",
    "- random_state=42 keeps results reproducible\n",
    "\n",
    "**Findings**\n",
    "- The model trains on balanced subsets to reduce bias toward the majority class\n",
    "- It helps improve minority class recognition\n",
    "- Ready to fit on the training data next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d5930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BalancedBaggingClassifier\n",
    "balanced_bagging_classifier = BalancedBaggingClassifier(base_classifier,\n",
    "                                                        sampling_strategy='auto',  # You can adjust this parameter\n",
    "                                                        replacement=False,  # Whether to sample with or without replacement\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600036c1",
   "metadata": {},
   "source": [
    "This code creates a BalancedBaggingClassifier by starting with a RandomForestClassifier that was previously defined. A random state is established for reproducibility, and options like \"sampling_strategy\" and \"replacement\" are supplied to address class imbalance.\n",
    "\n",
    "***Fitting the model and making predictions***\n",
    "\n",
    "**Observation**\n",
    "- The model is trained on X_train and y_train\n",
    "- Predictions are made on X_test\n",
    "- Output labels are stored in y_pred\n",
    "\n",
    "**Findings**\n",
    "- The model is now ready for evaluation using accuracy and classification report\n",
    "- y_pred contains the predicted class for each test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4579a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "balanced_bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = balanced_bagging_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f831331",
   "metadata": {},
   "source": [
    "This code use the training data (X_train, y_train) to train the BalancedBaggingClassifier. Then, using the test data (X_test), they predict the labels, saving the results in the variable y_pred.\n",
    "\n",
    "***Evaluation***\n",
    "\n",
    "**Observation**\n",
    "- Prints overall accuracy on the test set\n",
    "- Prints precision, recall, and F1-score for each class\n",
    "\n",
    "**Findings**\n",
    "- Accuracy shows overall correctness\n",
    "- The classification report shows how well the model identifies the minority class\n",
    "- Use these metrics to judge if imbalance is handled well, not accuracy alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d081832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00       187\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d02835",
   "metadata": {},
   "source": [
    "4. SMOTE\n",
    "The Synthetic Minority Oversampling Technique (SMOTE) addresses imbalanced datasets by synthetically generating new instances for the minority class. Unlike simply duplicating records, SMOTE enhances diversity by creating artificial instances. In simpler terms, SMOTE examines instances in the minority class, selects a random nearest neighbor using k-nearest neighbors, and generates a synthetic instance randomly within the feature space.\n",
    "\n",
    "**Observation**\n",
    "- Splits the dataset into train/test\n",
    "- Shows class counts before SMOTE\n",
    "- Applies SMOTE to balance the training set\n",
    "- Shows class counts after SMOTE\n",
    "\n",
    "**Findings**\n",
    "- Minority class is increased using synthetic samples\n",
    "- Training data becomes balanced, improving fairness in learning\n",
    "- Test data remains unchanged to avoid data leakage\n",
    "\n",
    "This code demonstrates how to rectify class imbalance in a dataset using SMOTE. Initially, an unbalanced dataset is produced, with 10% of the data belonging to a minority class. It shows the class distribution before to SMOTE after dividing the data into training and testing sets. After that, the minority class is oversampled using the SMOTE approach to produce synthetic instances. It displays a more equal representation of both classes in the resampled training data by printing the class distribution after SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8ca0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE: Counter({np.int64(1): 713, np.int64(0): 87})\n",
      "Class distribution after SMOTE: Counter({np.int64(1): 713, np.int64(0): 713})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d596e12",
   "metadata": {},
   "source": [
    "5. Threshold Moving\n",
    "\n",
    "In classifiers, predictions are often expressed as probabilities of class membership. The conventional threshold for assigning predictions to classes is typically set at 0.5. However, in the context of imbalanced class problems, this default threshold may not yield optimal results. To enhance classifier performance, it is essential to adjust the threshold to a value that efficiently discriminates between the two classes.\n",
    "\n",
    "Techniques such as ROC Curves and Precision-Recall Curves are employed to identify the optimal threshold. Additionally, grid search methods or exploration within a specified range of values can be utilized to pinpoint the most suitable threshold for the classifier.\n",
    "\n",
    "**Observation**\n",
    "- Trains a Random Forest and gets predicted probabilities\n",
    "- Iterates thresholds from 0.50 down to 0.00\n",
    "- Computes and prints F1-score at each threshold\n",
    "\n",
    "**Findings**\n",
    "- Threshold tuning can improve minority-class performance\n",
    "- Use the best F1-score threshold instead of default 0.5 for imbalanced data\n",
    "- Test set remains unchanged, avoiding leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cbf3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.50, F1 Score: 1.0000\n",
      "Threshold: 0.48, F1 Score: 1.0000\n",
      "Threshold: 0.46, F1 Score: 1.0000\n",
      "Threshold: 0.44, F1 Score: 1.0000\n",
      "Threshold: 0.42, F1 Score: 1.0000\n",
      "Threshold: 0.40, F1 Score: 1.0000\n",
      "Threshold: 0.38, F1 Score: 1.0000\n",
      "Threshold: 0.36, F1 Score: 1.0000\n",
      "Threshold: 0.34, F1 Score: 1.0000\n",
      "Threshold: 0.32, F1 Score: 1.0000\n",
      "Threshold: 0.30, F1 Score: 1.0000\n",
      "Threshold: 0.28, F1 Score: 0.9973\n",
      "Threshold: 0.26, F1 Score: 0.9973\n",
      "Threshold: 0.24, F1 Score: 0.9973\n",
      "Threshold: 0.22, F1 Score: 0.9947\n",
      "Threshold: 0.20, F1 Score: 0.9947\n",
      "Threshold: 0.18, F1 Score: 0.9947\n",
      "Threshold: 0.16, F1 Score: 0.9920\n",
      "Threshold: 0.14, F1 Score: 0.9920\n",
      "Threshold: 0.12, F1 Score: 0.9894\n",
      "Threshold: 0.10, F1 Score: 0.9842\n",
      "Threshold: 0.08, F1 Score: 0.9740\n",
      "Threshold: 0.06, F1 Score: 0.9664\n",
      "Threshold: 0.04, F1 Score: 0.9664\n",
      "Threshold: 0.02, F1 Score: 0.9664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model (Random Forest as an example)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Set a threshold (initially 0.5)\n",
    "threshold = 0.5\n",
    "\n",
    "# Adjust threshold based on your criteria (e.g., maximizing F1-score)\n",
    "while threshold >= 0:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Threshold: {threshold:.2f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Move the threshold (you can customize the step size)\n",
    "    threshold -= 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294e612",
   "metadata": {},
   "source": [
    "6. Using Tree Based Models\n",
    "The hierarchical structure of tree-based models—such as Decision Trees, Random Forests, and Gradient Boosted Trees—allows them to handle imbalanced datasets better than non-tree-based models.\n",
    "\n",
    "- Decision Trees: Decision trees create a structure resembling a tree by dividing the feature space into regions according to feature values. By changing the decision boundaries to incorporate minority class patterns, decision trees can react to data that is unbalanced. They might experience overfitting, though.\n",
    "- Random Forests: Random Forests are made up of many Decision Trees that have been trained using arbitrary subsets of features and data. Random Forests improve generalization by reducing overfitting and strengthening robustness against imbalanced datasets by mixing numerous trees.\n",
    "- Gradient Boosted Trees: Boosted Gradient Trees grow in a sequential fashion, with each new growth repairing the mistakes of the older one. Gradient Boosted Trees perform well in imbalanced circumstances because of their ability to concentrate on misclassified occurrences through sequential learning. Although they often work effectively, they could be noise-sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfec52",
   "metadata": {},
   "source": [
    "7. Using Anomaly Detection Algorithms\n",
    "- Anomaly or Outlier Detection algorithms are 'one class classification algorithms' that helps in identifying outliers ( rare data points) in the dataset.\n",
    "- In an Imbalanced dataset, assume  'Majority class records as Normal data' and 'Minority Class records as Outlier data'.\n",
    "- These algorithms are trained on Normal data.\n",
    "- A trained model can predict if the new record is Normal or Outlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
