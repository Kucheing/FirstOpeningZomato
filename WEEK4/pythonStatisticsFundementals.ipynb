{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4220fc",
   "metadata": {},
   "source": [
    "# **Python Statistics Fundamentals: How to Describe Your Data**\n",
    "\n",
    "# Understanding Descriptive Statistics\n",
    "**Descriptive statistics** is about describing and summarizing data. It uses two main approaches:\n",
    "\n",
    "**The quantitative approach** describes and summarizes data numerically.\n",
    "**The visual approach** illustrates data with charts, plots, histograms, and other graphs.\n",
    "\n",
    "You can apply descriptive statistics to one or many datasets or variables. When you describe and summarize a single variable, you‚Äôre performing **univariate analysis**. When you search for statistical relationships among a pair of variables, you‚Äôre doing a **bivariate analysis**. Similarly, a **multivariate analysis** is concerned with multiple variables at once.\n",
    "\n",
    "# Types of Measures\n",
    "\n",
    "In this tutorial, you‚Äôll learn about the following types of measures in descriptive statistics:\n",
    "\n",
    "- **Central tendency** tells you about the centers of the data. Useful measures include the mean, median, and mode.\n",
    "- **Variability** tells you about the spread of the data. Useful measures include variance and standard deviation.\n",
    "- **Correlation or joint variability** tells you about the relation between a pair of variables in a dataset. Useful measures include covariance and the correlation coefficient.\n",
    "\n",
    "# Population and Samples\n",
    "In statistics, the **population** is a set of all elements or items that you‚Äôre interested in. Populations are often vast, which makes them inappropriate for collecting and analyzing data. That‚Äôs why statisticians usually try to make some conclusions about a population by choosing and examining a representative subset of that population.\n",
    "\n",
    "This subset of a population is called a **sample**. Ideally, the sample should preserve the essential statistical features of the population to a satisfactory extent. That way, you‚Äôll be able to use the sample to glean conclusions about the population.\n",
    "\n",
    "# Outliers\n",
    "An **outlier** is a data point that differs significantly from the majority of the data taken from a sample or population. There are many possible causes of outliers, but here are a few to start you off:\n",
    "\n",
    "- Natural variation in data\n",
    "- Change in the behavior of the observed system\n",
    "- Errors in data collection\n",
    "\n",
    "Data collection errors are a particularly prominent cause of outliers. For example, the limitations of measurement instruments or procedures can mean that the correct data is simply not obtainable. Other errors can be caused by miscalculations, data contamination, human error, and more.\n",
    "\n",
    "There isn‚Äôt a precise mathematical definition of outliers. You have to rely on experience, knowledge about the subject of interest, and common sense to determine if a data point is an outlier and how to handle it.\n",
    "\n",
    "# Choosing Python Statistics Libraries\n",
    "\n",
    "There are many Python statistics libraries out there for you to work with, but in this tutorial, you‚Äôll be learning about some of the most popular and widely used ones:\n",
    "\n",
    "- **Python‚Äôs** statistics is a built-in Python library for descriptive statistics. You can use it if your datasets are not too large or if you can‚Äôt rely on importing other libraries.\n",
    "\n",
    "- **NumPy** is a third-party library for numerical computing, optimized for working with single- and multi-dimensional arrays. Its primary type is the array type called ndarray. This library contains many routines for statistical analysis.\n",
    "\n",
    "- **SciPy** is a third-party library for scientific computing based on NumPy. It offers additional functionality compared to NumPy, including scipy.stats for statistical analysis.\n",
    "\n",
    "- **pandas** is a third-party library for numerical computing based on NumPy. It excels in handling labeled one-dimensional (1D) data with Series objects and two-dimensional (2D) data with DataFrame objects.\n",
    "\n",
    "- **Matplotlib** is a third-party library for data visualization. It works well in combination with NumPy, SciPy, and pandas.\n",
    "\n",
    "Note that, in many cases, Series and DataFrame objects can be used in place of NumPy arrays. Often, you might just pass them to a NumPy or SciPy statistical function. In addition, you can get the unlabeled data from a Series or DataFrame as a np.ndarray object by calling .values or .to_numpy().\n",
    "\n",
    "# Getting Started With Python Statistics Libraries\n",
    "\n",
    "The built-in Python statistics library has a relatively small number of the most important statistics functions. The official documentation is a valuable resource to find the details. If you‚Äôre limited to pure Python, then the Python statistics library might be the right choice.\n",
    "\n",
    "A good place to start learning about NumPy is the official User Guide, especially the quickstart and basics sections. The official reference can help you refresh your memory on specific NumPy concepts. While you read this tutorial, you might want to check out the statistics section and the official scipy.stats reference as well.\n",
    "\n",
    "If you want to learn pandas, then the official Getting Started page is an excellent place to begin. The introduction to data structures can help you learn about the fundamental data types, Series and DataFrame. Likewise, the excellent official introductory tutorial aims to give you enough information to start effectively using pandas in practice.\n",
    "\n",
    "matplotlib has a comprehensive official User‚Äôs Guide that you can use to dive into the details of using the library. Anatomy of Matplotlib is an excellent resource for beginners who want to start working with matplotlib and its related libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2aa919",
   "metadata": {},
   "source": [
    "# Calculating Descriptive Statistics\n",
    "Start by importing all the packages you‚Äôll need:\n",
    "\n",
    "These are all the packages you‚Äôll need for Python statistics calculations. Usually, you won‚Äôt use Python‚Äôs built-in math package, but it‚Äôll be useful in this tutorial. Later, you‚Äôll import matplotlib.pyplot for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c262d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a28e0",
   "metadata": {},
   "source": [
    "Let‚Äôs create some data to work with. You‚Äôll start with Python lists that contain some arbitrary numeric data:\n",
    "\n",
    "Now you have the lists x and x_with_nan. They‚Äôre almost the same, with the difference that x_with_nan contains a nan value. It‚Äôs important to understand the behavior of the Python statistics routines when they come across a not-a-number value (nan). In data science, missing values are common, and you‚Äôll often replace them with nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb4365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 1, 2.5, 4, 28.0]\n",
      "[8.0, 1, 2.5, nan, 4, 28.0]\n"
     ]
    }
   ],
   "source": [
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "x_with_nan = [8.0, 1, 2.5, math.nan, 4, 28.0]\n",
    "print(x)\n",
    "print(x_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39436479",
   "metadata": {},
   "source": [
    "Now, create np.ndarray and pd.Series objects that correspond to x and x_with_nan:\n",
    "\n",
    "You now have two NumPy arrays (y and y_with_nan) and two pandas Series (z and z_with_nan). All of these are 1D sequences of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48fb47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array [ 8.   1.   2.5  4.  28. ]\n",
      "array [ 8.   1.   2.5  nan  4.  28. ]\n",
      "0     8.0\n",
      "1     1.0\n",
      "2     2.5\n",
      "3     4.0\n",
      "4    28.0\n",
      "dtype: float64\n",
      "0     8.0\n",
      "1     1.0\n",
      "2     2.5\n",
      "3     NaN\n",
      "4     4.0\n",
      "5    28.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y, y_with_nan = np.array(x), np.array(x_with_nan)\n",
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)\n",
    "print(\"array\",y)\n",
    "print(\"array\",y_with_nan)\n",
    "print(z)\n",
    "\n",
    "print(z_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4aaa",
   "metadata": {},
   "source": [
    "# Measures of Central Tendency\n",
    "The **measures of central tendency** show the central or middle values of datasets. There are several definitions of what‚Äôs considered to be the center of a dataset. In this tutorial, you‚Äôll learn how to identify and calculate these measures of central tendency:\n",
    "\n",
    "- Mean\n",
    "- Weighted mean\n",
    "- Geometric mean\n",
    "- Harmonic mean\n",
    "- Median\n",
    "- Mode\n",
    "\n",
    "# Mean\n",
    "The **sample mean**, also called the **sample arithmetic mean** or simply the **average**, is the arithmetic average of all the items in a dataset. The mean of a dataset ùë• is mathematically expressed as Œ£·µ¢ùë•·µ¢/ùëõ, where ùëñ = 1, 2, ‚Ä¶, ùëõ. In other words, it‚Äôs the sum of all the elements ùë•·µ¢ divided by the number of items in the dataset ùë•.\n",
    "\n",
    "Although this is clean and elegant, you can also apply built-in Python statistics functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68846481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1, 2.5, 4, 8, 28\n",
    "mean_ = sum(x) / len(x)\n",
    "print(mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c47bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.7\n",
      "8.7\n"
     ]
    }
   ],
   "source": [
    "mean_ = statistics.mean(x)\n",
    "print(mean_)\n",
    "mean_ = statistics.fmean(x)\n",
    "print(mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec539659",
   "metadata": {},
   "source": [
    "You‚Äôve called the functions mean() and fmean() from the built-in Python statistics library and got the same result as you did with pure Python. fmean() is introduced in Python 3.8 as a faster alternative to mean(). It always returns a floating-point number.\n",
    "\n",
    "However, if there are nan values among your data, then statistics.mean() and statistics.fmean() will return nan as the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfd51ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "mean_ = statistics.mean(x_with_nan)\n",
    "print(mean_)\n",
    "mean_ = statistics.fmean(x_with_nan)\n",
    "print(mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8728c0",
   "metadata": {},
   "source": [
    "This result is consistent with the behavior of sum(), because sum(x_with_nan) also returns nan.\n",
    "\n",
    "If you use NumPy, then you can get the mean with np.mean():\n",
    "\n",
    "In the example above, mean() is a function, but you can use the corresponding method .mean() as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a3d9623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ = np.mean(y)\n",
    "mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcc91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ = y.mean()\n",
    "mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d18afd",
   "metadata": {},
   "source": [
    "The function mean() and method .mean() from NumPy return the same result as statistics.mean(). This is also the case when there are nan values among your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69718c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(nan)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_with_nan)\n",
    "\n",
    "y_with_nan.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0911f0a",
   "metadata": {},
   "source": [
    "You often don‚Äôt need to get a nan value as a result. If you prefer to ignore nan values, then you can use np.nanmean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31371e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(y_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0206eca",
   "metadata": {},
   "source": [
    "nanmean() simply ignores all nan values. It returns the same value as mean() if you were to apply it to the dataset without the nan values.\n",
    "\n",
    "pd.Series objects also have the method .mean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d56348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ = z.mean()\n",
    "mean_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c208fbf",
   "metadata": {},
   "source": [
    "As you can see, it‚Äôs used similarly as in the case of NumPy. However, .mean() from pandas ignores nan values by default:\n",
    "\n",
    "This behavior is the result of the default value of the optional parameter skipna. You can change this parameter to modify the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b12f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_with_nan.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631251f",
   "metadata": {},
   "source": [
    "# Weighted Mean\n",
    "The **weighted mean**, also called the **weighted arithmetic mean** or **weighted average**, is a generalization of the arithmetic mean that enables you to define the relative contribution of each data point to the result.\n",
    "\n",
    "You define one **weight ùë§·µ¢** for each data point ùë•·µ¢ of the dataset ùë•, where ùëñ = 1, 2, ‚Ä¶, ùëõ and ùëõ is the number of items in ùë•. Then, you multiply each data point with the corresponding weight, sum all the products, and divide the obtained sum with the sum of weights: Œ£·µ¢(ùë§·µ¢ùë•·µ¢) / Œ£·µ¢ùë§·µ¢.\n",
    "\n",
    "Note: It‚Äôs convenient (and usually the case) that all weights are nonnegative, ùë§·µ¢ ‚â• 0, and that their sum is equal to one, or Œ£·µ¢ùë§·µ¢ = 1.\n",
    "\n",
    "The weighted mean is very handy when you need the mean of a dataset containing items that occur with given relative frequencies. For example, say that you have a set in which 20% of all items are equal to 2, 50% of the items are equal to 4, and the remaining 30% of the items are equal to 8. You can calculate the mean of such a set like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070f1109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 * 2 + 0.5 * 4 + 0.3 * 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57c923",
   "metadata": {},
   "source": [
    "Here, you take the frequencies into account with the weights. With this method, you don‚Äôt need to know the total number of items.\n",
    "\n",
    "You can implement the weighted mean in pure Python by combining sum() with either range() or zip():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db97c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.95\n",
      "6.95\n"
     ]
    }
   ],
   "source": [
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "w = [0.1, 0.2, 0.3, 0.25, 0.15]\n",
    "wmean = sum(w[i] * x[i] for i in range(len(x))) / sum(w)\n",
    "print(wmean)\n",
    "\n",
    "wmean = sum(x_ * w_ for (x_, w_) in zip(x, w)) / sum(w)\n",
    "print(wmean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3c056",
   "metadata": {},
   "source": [
    "Again, this is a clean and elegant implementation where you don‚Äôt need to import any libraries.\n",
    "\n",
    "However, if you have large datasets, then NumPy is likely to provide a better solution. You can use np.average() to get the weighted mean of NumPy arrays or pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e6b914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.95\n",
      "6.95\n"
     ]
    }
   ],
   "source": [
    "y, z, w = np.array(x), pd.Series(x), np.array(w)\n",
    "wmean = np.average(y, weights=w)\n",
    "print(wmean)\n",
    "\n",
    "wmean = np.average(z, weights=w)\n",
    "print(wmean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce3673",
   "metadata": {},
   "source": [
    "The result is the same as in the case of the pure Python implementation. You can also use this method on ordinary lists and tuples.\n",
    "\n",
    "Another solution is to use the element-wise product w * y with np.sum() or .sum():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad9eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.95)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w * y).sum() / w.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1d6fd",
   "metadata": {},
   "source": [
    "That‚Äôs it! You‚Äôve calculated the weighted mean.\n",
    "\n",
    "However, be careful if your dataset contains nan values:\n",
    "\n",
    "In this case, average() returns nan, which is consistent with np.mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0ffcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.1, 0.2, 0.3, 0.0, 0.2, 0.1])\n",
    "print((w * y_with_nan).sum() / w.sum())\n",
    "\n",
    "print(np.average(y_with_nan, weights=w))\n",
    "\n",
    "print(np.average(z_with_nan, weights=w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c483a55",
   "metadata": {},
   "source": [
    "# Harmonic Mean\n",
    "\n",
    "The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset: ùëõ / Œ£·µ¢(1/ùë•·µ¢), where ùëñ = 1, 2, ‚Ä¶, ùëõ and ùëõ is the number of items in the dataset ùë•. One variant of the pure Python implementation of the harmonic mean is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8f2a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmean = len(x) / sum(1 / item for item in x)\n",
    "hmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfaf81",
   "metadata": {},
   "source": [
    "It‚Äôs quite different from the value of the arithmetic mean for the same data x, which you calculated to be 8.7.\n",
    "\n",
    "You can also calculate this measure with statistics.harmonic_mean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62a7b915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmean = statistics.harmonic_mean(x)\n",
    "hmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34446b2",
   "metadata": {},
   "source": [
    "The example above shows one implementation of statistics.harmonic_mean(). If you have a nan value in a dataset, then it‚Äôll return nan. If there‚Äôs at least one 0, then it‚Äôll return 0. If you provide at least one negative number, then you‚Äôll get statistics.StatisticsError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cea1e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "harmonic mean does not support negative values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStatisticsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(statistics.harmonic_mean(x_with_nan))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(statistics.harmonic_mean([\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m]))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mstatistics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mharmonic_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raises StatisticsError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\statistics.py:593\u001b[39m, in \u001b[36mharmonic_mean\u001b[39m\u001b[34m(data, weights)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    592\u001b[39m     data = _fail_neg(data, errmsg)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     T, total, count = \u001b[43m_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m:\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\statistics.py:193\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m typ, values \u001b[38;5;129;01min\u001b[39;00m groupby(data, \u001b[38;5;28mtype\u001b[39m):\n\u001b[32m    192\u001b[39m     types_add(typ)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_exact_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartials\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartials_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\statistics.py:593\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    592\u001b[39m     data = _fail_neg(data, errmsg)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     T, total, count = _sum\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m:\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\statistics.py:355\u001b[39m, in \u001b[36m_fail_neg\u001b[39m\u001b[34m(values, errmsg)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x < \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m StatisticsError(errmsg)\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m x\n",
      "\u001b[31mStatisticsError\u001b[39m: harmonic mean does not support negative values"
     ]
    }
   ],
   "source": [
    "print(statistics.harmonic_mean(x_with_nan))\n",
    "\n",
    "print(statistics.harmonic_mean([1, 0, 2]))\n",
    "\n",
    "statistics.harmonic_mean([1, 2, -2])  # Raises StatisticsError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42383eb0",
   "metadata": {},
   "source": [
    "Keep these three scenarios in mind when you‚Äôre using this method!\n",
    "\n",
    "A third way to calculate the harmonic mean is to use scipy.stats.hmean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab6a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7613412228796843\n",
      "2.7613412228796843\n"
     ]
    }
   ],
   "source": [
    "print(scipy.stats.hmean(y))\n",
    "print(scipy.stats.hmean(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3a1fc",
   "metadata": {},
   "source": [
    "Again, this is a pretty straightforward implementation. However, if your dataset contains nan, 0, a negative number, or anything but positive numbers, then you‚Äôll get a ValueError!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a47f9",
   "metadata": {},
   "source": [
    "# Geometric Mean\n",
    "The **geometric mean** is the ùëõ-th root of the product of all ùëõ elements ùë•·µ¢ in a dataset ùë•: ‚Åø‚àö(Œ†·µ¢ùë•·µ¢), where ùëñ = 1, 2, ‚Ä¶, ùëõ. The following figure illustrates the arithmetic, harmonic, and geometric means of a dataset:\n",
    "\n",
    "Again, the green dots represent the data points 1, 2.5, 4, 8, and 28. The red dashed line is the mean. The blue dashed line is the harmonic mean, and the yellow dashed line is the geometric mean.\n",
    "\n",
    "You can implement the geometric mean in pure Python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7412bcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.677885674856041"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmean = 1\n",
    "for item in x:\n",
    "    gmean *= item\n",
    "\n",
    "gmean **= 1 / len(x)\n",
    "gmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e730a",
   "metadata": {},
   "source": [
    "As you can see, the value of the geometric mean, in this case, differs significantly from the values of the arithmetic (8.7) and harmonic (2.76) means for the same dataset x.\n",
    "\n",
    "Python 3.8 introduced statistics.geometric_mean(), which converts all values to floating-point numbers and returns their geometric mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5eb7b6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.67788567485604"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmean = statistics.geometric_mean(x)\n",
    "gmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48826a89",
   "metadata": {},
   "source": [
    "You‚Äôve got the same result as in the previous example, but with a minimal rounding error.\n",
    "\n",
    "If you pass data with nan values, then statistics.geometric_mean() will behave like most similar functions and return nan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff6ceb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmean = statistics.geometric_mean(x_with_nan)\n",
    "gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1286cfb",
   "metadata": {},
   "source": [
    "Indeed, this is consistent with the behavior of statistics.mean(), statistics.fmean(), and statistics.harmonic_mean(). If there‚Äôs a zero or negative number among your data, then statistics.geometric_mean() will raise the statistics.StatisticsError.\n",
    "\n",
    "You can also get the geometric mean with scipy.stats.gmean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c34fcd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.67788567485604)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.gmean(y)\n",
    "scipy.stats.gmean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bf0ed",
   "metadata": {},
   "source": [
    "You obtained the same result as with the pure Python implementation.\n",
    "\n",
    "If you have nan values in a dataset, then gmean() will return nan. If there‚Äôs at least one 0, then it‚Äôll return 0.0 and give a warning. If you provide at least one negative number, then you‚Äôll get nan and the warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cec4f",
   "metadata": {},
   "source": [
    "# Median\n",
    "The **sample median** is the middle element of a sorted dataset. The dataset can be sorted in increasing or decreasing order. If the number of elements ùëõ of the dataset is odd, then the median is the value at the middle position: 0.5(ùëõ + 1). If ùëõ is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5ùëõ and 0.5ùëõ + 1.\n",
    "\n",
    "For example, if you have the data points 2, 4, 1, 8, and 9, then the median value is 4, which is in the middle of the sorted dataset (1, 2, 4, 8, 9). If the data points are 2, 4, 1, and 8, then the median is 3, which is the average of the two middle elements of the sorted sequence (2 and 4). The following figure illustrates this:\n",
    "\n",
    "The data points are the green dots, and the purple lines show the median for each dataset. The median value for the upper dataset (1, 2.5, 4, 8, and 28) is 4. If you remove the outlier 28 from the lower dataset, then the median becomes the arithmetic average between 2.5 and 4, which is 3.25.\n",
    "\n",
    "The figure below shows both the mean and median of the data points 1, 2.5, 4, 8, and 28:\n",
    "\n",
    "Again, the mean is the red dashed line, while the median is the purple line.\n",
    "\n",
    "The main difference between the behavior of the mean and median is related to dataset outliers or extremes. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all. Consider the following figure:\n",
    "\n",
    "The upper dataset again has the items 1, 2.5, 4, 8, and 28. Its mean is 8.7, and the median is 5, as you saw earlier. The lower dataset shows what‚Äôs going on when you move the rightmost point with the value 28:\n",
    "\n",
    "If you increase its value (move it to the right), then the mean will rise, but the median value won‚Äôt ever change.\n",
    "If you decrease its value (move it to the left), then the mean will drop, but the median will remain the same until the value of the moving point is greater than or equal to 4.\n",
    "You can compare the mean and median as one way to detect outliers and asymmetry in your data. Whether the mean value or the median value is more useful to you depends on the context of your particular problem.\n",
    "\n",
    "Here is one of many possible pure Python implementations of the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "n = len(x)\n",
    "if n % 2:\n",
    "    median_ = sorted(x)[round(0.5*(n-1))]\n",
    "else:\n",
    "    x_ord, index = sorted(x), round(0.5 * n)\n",
    "    median_ = 0.5 * (x_ord[index-1] + x_ord[index])\n",
    "\n",
    "print(median_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a471090",
   "metadata": {},
   "source": [
    "Two most important steps of this implementation are as follows:\n",
    "\n",
    "Sorting the elements of the dataset\n",
    "Finding the middle element(s) in the sorted dataset\n",
    "You can get the median with statistics.median():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccb4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_ = statistics.median(x)\n",
    "print(median_)\n",
    "\n",
    "median_ = statistics.median(x[:-1])\n",
    "print(median_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605cc67",
   "metadata": {},
   "source": [
    "The sorted version of x is [1, 2.5, 4, 8.0, 28.0], so the element in the middle is 4. The sorted version of x[:-1], which is x without the last item 28.0, is [1, 2.5, 4, 8.0]. Now, there are two middle elements, 2.5 and 4. Their average is 3.25.\n",
    "\n",
    "median_low() and median_high() are two more functions related to the median in the Python statistics library. They always return an element from the dataset:\n",
    "\n",
    "- If the number of elements is odd, then there‚Äôs a single middle value, so these functions behave just like median().\n",
    "- If the number of elements is even, then there are two middle values. In this case, median_low() returns the lower and median_high() the higher middle value.\n",
    "\n",
    "You can use these functions just as you‚Äôd use median():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c4be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(statistics.median_low(x[:-1]))\n",
    "\n",
    "print(statistics.median_high(x[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d429642",
   "metadata": {},
   "source": [
    "Again, the sorted version of x[:-1] is [1, 2.5, 4, 8.0]. The two elements in the middle are 2.5 (low) and 4 (high).\n",
    "\n",
    "Unlike most other functions from the Python statistics library, median(), median_low(), and median_high() don‚Äôt return nan when there are nan values among the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eac819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "4\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(statistics.median(x_with_nan))\n",
    "\n",
    "print(statistics.median_low(x_with_nan))\n",
    "\n",
    "print(statistics.median_high(x_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b3d14",
   "metadata": {},
   "source": [
    "Beware of this behavior because it might not be what you want!\n",
    "\n",
    "You can also get the median with np.median():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49bec9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "3.25\n"
     ]
    }
   ],
   "source": [
    "median_ = np.median(y)\n",
    "print(median_)\n",
    "\n",
    "median_ = np.median(y[:-1])\n",
    "print(median_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b2ce2",
   "metadata": {},
   "source": [
    "You‚Äôve obtained the same values with statistics.median() and np.median().\n",
    "\n",
    "However, if there‚Äôs a nan value in your dataset, then np.median() issues the RuntimeWarning and returns nan. If this behavior is not what you want, then you can use nanmedian() to ignore all nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a417f25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmedian(y_with_nan)\n",
    "\n",
    "np.nanmedian(y_with_nan[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f4ee5",
   "metadata": {},
   "source": [
    "The obtained results are the same as with statistics.median() and np.median() applied to the datasets x and y.\n",
    "\n",
    "pandas Series objects have the method .median() that ignores nan values by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "567f0d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(z.median())\n",
    "\n",
    "print(z_with_nan.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32e0a4",
   "metadata": {},
   "source": [
    "The behavior of .median() is consistent with .mean() in pandas. You can change this behavior with the optional parameter skipna.\n",
    "\n",
    "# Mode\n",
    "The **sample mode** is the value in the dataset that occurs most frequently. If there isn‚Äôt a single such value, then the set is **multimodal** since it has multiple modal values. For example, in the set that contains the points 2, 3, 2, 8, and 12, the number 2 is the mode because it occurs twice, unlike the other items that occur only once.\n",
    "\n",
    "This is how you can get the mode with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593cac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = [2, 3, 2, 8, 12]\n",
    "mode_ = max((u.count(item), item) for item in set(u))[1]\n",
    "mode_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acddf36",
   "metadata": {},
   "source": [
    "You use u.count() to get the number of occurrences of each item in u. The item with the maximal number of occurrences is the mode. Note that you don‚Äôt have to use set(u). Instead, you might replace it with just u and iterate over the entire list.\n",
    "\n",
    "You can obtain the mode with statistics.mode() and statistics.multimode():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1d91b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_ = statistics.mode(u)\n",
    "mode_\n",
    "mode_ = statistics.multimode(u)\n",
    "mode_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c9ccb",
   "metadata": {},
   "source": [
    "As you can see, mode() returned a single value, while multimode() returned the list that contains the result. This isn‚Äôt the only difference between the two functions, though. If there‚Äôs more than one modal value, then mode() raises StatisticsError, while multimode() returns the list with all modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59a0b00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[12, 15]\n"
     ]
    }
   ],
   "source": [
    "v = [12, 15, 12, 15, 21, 15, 12]\n",
    "print(statistics.mode(v))  # Raises StatisticsError\n",
    "print(statistics.multimode(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f215b28",
   "metadata": {},
   "source": [
    "You should pay special attention to this scenario and be careful when you‚Äôre choosing between these two functions.\n",
    "\n",
    "statistics.mode() and statistics.multimode() handle nan values as regular values and can return nan as the modal value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c53df79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[2]\n",
      "nan\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mode([2, math.nan, 2]))\n",
    "print(statistics.multimode([2, math.nan, 2]))\n",
    "print(statistics.mode([2, math.nan, 0, math.nan, 5]))\n",
    "print(statistics.multimode([2, math.nan, 0, math.nan, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb842d2",
   "metadata": {},
   "source": [
    "In the first example above, the number 2 occurs twice and is the modal value. In the second example, nan is the modal value since it occurs twice, while the other values occur only once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd4f7b",
   "metadata": {},
   "source": [
    "You can also get the mode with scipy.stats.mode():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f95bdda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModeResult(mode=np.int64(2), count=np.int64(2))\n",
      "ModeResult(mode=np.int64(12), count=np.int64(3))\n"
     ]
    }
   ],
   "source": [
    "u, v = np.array(u), np.array(v)\n",
    "mode_ = scipy.stats.mode(u)\n",
    "print(mode_)\n",
    "mode_ = scipy.stats.mode(v)\n",
    "print(mode_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c962cc7",
   "metadata": {},
   "source": [
    "This function returns the object with the modal value and the number of times it occurs. If there are multiple modal values in the dataset, then only the smallest value is returned.\n",
    "\n",
    "You can get the mode and its number of occurrences as NumPy arrays with dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0eda31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(mode_.mode)\n",
    "print(mode_.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4528c9d",
   "metadata": {},
   "source": [
    "This code uses .mode to return the smallest mode (12) in the array v and .count to return the number of times it occurs (3). scipy.stats.mode() is also flexible with nan values. It allows you to define desired behavior with the optional parameter nan_policy. This parameter can take on the values 'propagate', 'raise' (an error), or 'omit'.\n",
    "\n",
    "pandas Series objects have the method .mode() that handles multimodal values well and ignores nan values by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe427a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "dtype: int64\n",
      "0    12\n",
      "1    15\n",
      "dtype: int64\n",
      "0    2.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "u, v, w = pd.Series(u), pd.Series(v), pd.Series([2, 2, math.nan])\n",
    "print(u.mode())\n",
    "\n",
    "print(v.mode())\n",
    "\n",
    "print(w.mode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad5f1e",
   "metadata": {},
   "source": [
    "As you can see, .mode() returns a new pd.Series that holds all modal values. If you want .mode() to take nan values into account, then just pass the optional argument dropna=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03324c15",
   "metadata": {},
   "source": [
    "# Measures of Variability\n",
    "The measures of central tendency aren‚Äôt sufficient to describe data. You‚Äôll also need the **measures of variability** that quantify the spread of data points. In this section, you‚Äôll learn how to identify and calculate the following variability measures:\n",
    "\n",
    "- Variance\n",
    "- Standard deviation\n",
    "- Skewness\n",
    "- Percentiles\n",
    "- Ranges\n",
    "\n",
    "# Variance\n",
    "The **sample variance** quantifies the spread of the data. It shows numerically how far the data points are from the mean. You can express the sample variance of the dataset ùë• with ùëõ elements mathematically as ùë†¬≤ = Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≤ / (ùëõ ‚àí 1), where ùëñ = 1, 2, ‚Ä¶, ùëõ and mean(ùë•) is the sample mean of ùë•. If you want to understand deeper why you divide the sum with ùëõ ‚àí 1 instead of ùëõ, then you can dive deeper into Bessel‚Äôs correction.\n",
    "\n",
    "The following figure shows you why it‚Äôs important to consider the variance when describing datasets:\n",
    "\n",
    "There are two datasets in this figure:\n",
    "\n",
    "- Green dots: This dataset has a smaller variance or a smaller average difference from the mean. It also has a smaller range or a smaller difference between the largest and smallest item.\n",
    "- White dots: This dataset has a larger variance or a larger average difference from the mean. It also has a bigger range or a bigger difference between the largest and smallest item.\n",
    "Note that these two datasets have the same mean and median, even though they appear to differ significantly. Neither the mean nor the median can describe this difference. That‚Äôs why you need the measures of variability.\n",
    "\n",
    "Here‚Äôs how you can calculate the sample variance with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba9d345d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(x)\n",
    "mean_ = sum(x) / n\n",
    "var_ = sum((item - mean_)**2 for item in x) / (n - 1)\n",
    "var_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d654ae3",
   "metadata": {},
   "source": [
    "This approach is sufficient and calculates the sample variance well. However, the shorter and more elegant solution is to call the existing function statistics.variance():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68a9686a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_ = statistics.variance(x)\n",
    "var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34920e8",
   "metadata": {},
   "source": [
    "You‚Äôve obtained the same result for the variance as above. variance() can avoid calculating the mean if you provide the mean explicitly as the second argument: statistics.variance(x, mean_).\n",
    "\n",
    "If you have nan values among your data, then statistics.variance() will return nan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f8362de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.variance(x_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1628b",
   "metadata": {},
   "source": [
    "This behavior is consistent with mean() and most other functions from the Python statistics library.\n",
    "\n",
    "You can also calculate the sample variance with NumPy. You should use the function np.var() or the corresponding method .var():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3387f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.19999999999999\n",
      "123.19999999999999\n"
     ]
    }
   ],
   "source": [
    "var_ = np.var(y, ddof=1)\n",
    "print(var_)\n",
    "\n",
    "var_ = y.var(ddof=1)\n",
    "print(var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5674ed2",
   "metadata": {},
   "source": [
    "It‚Äôs very important to specify the parameter ddof=1. That‚Äôs how you set the delta degrees of freedom to 1. This parameter allows the proper calculation of ùë†¬≤, with (ùëõ ‚àí 1) in the denominator instead of ùëõ.\n",
    "\n",
    "If you have nan values in the dataset, then np.var() and .var() will return nan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfd7cf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(nan)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(y_with_nan, ddof=1)\n",
    "y_with_nan.var(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9de18",
   "metadata": {},
   "source": [
    "This is consistent with np.mean() and np.average(). If you want to skip nan values, then you should use np.nanvar():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79258e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(123.19999999999999)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanvar(y_with_nan, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977fb9f",
   "metadata": {},
   "source": [
    "np.nanvar() ignores nan values. It also needs you to specify ddof=1.\n",
    "\n",
    "pd.Series objects have the method .var() that skips nan values by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "818630e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.19999999999999\n",
      "123.19999999999999\n"
     ]
    }
   ],
   "source": [
    "print(z.var(ddof=1))\n",
    "print(z_with_nan.var(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289be699",
   "metadata": {},
   "source": [
    "It also has the parameter ddof, but its default value is 1, so you can omit it. If you want a different behavior related to nan values, then use the optional parameter skipna.\n",
    "\n",
    "You calculate the population variance similarly to the sample variance. However, you have to use ùëõ in the denominator instead of ùëõ ‚àí 1: Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≤ / ùëõ. In this case, ùëõ is the number of items in the entire population. You can get the population variance similar to the sample variance, with the following differences:\n",
    "\n",
    "Replace (n - 1) with n in the pure Python implementation.\n",
    "Use statistics.pvariance() instead of statistics.variance().\n",
    "Specify the parameter ddof=0 if you use NumPy or pandas. In NumPy, you can omit ddof because its default value is 0.\n",
    "Note that you should always be aware of whether you‚Äôre working with a sample or the entire population whenever you‚Äôre calculating the variance!\n",
    "\n",
    "# Standard Deviation\n",
    "The **sample standard deviation** is another measure of data spread. It‚Äôs connected to the sample variance, as standard deviation, ùë†, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. Once you get the variance, you can calculate the standard deviation with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "024c3836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(11.099549540409285)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_ = var_ ** 0.5\n",
    "std_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197fa5a",
   "metadata": {},
   "source": [
    "Although this solution works, you can also use statistics.stdev():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c78d00a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.099549540409287"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_ = statistics.stdev(x)\n",
    "std_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c1e46",
   "metadata": {},
   "source": [
    "Of course, the result is the same as before. Like variance(), stdev() doesn‚Äôt calculate the mean if you provide it explicitly as the second argument: statistics.stdev(x, mean_).\n",
    "\n",
    "You can get the standard deviation with NumPy in almost the same way. You can use the function std() and the corresponding method .std() to calculate the standard deviation. If there are nan values in the dataset, then they‚Äôll return nan. To ignore nan values, you should use np.nanstd(). You use std(), .std(), and nanstd() from NumPy as you would use var(), .var(), and nanvar():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "964b730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.099549540409285\n",
      "11.099549540409285\n",
      "nan\n",
      "nan\n",
      "11.099549540409285\n"
     ]
    }
   ],
   "source": [
    "print(np.std(y, ddof=1))\n",
    "\n",
    "print(y.std(ddof=1))\n",
    "\n",
    "print(np.std(y_with_nan, ddof=1))\n",
    "\n",
    "print(y_with_nan.std(ddof=1))\n",
    "\n",
    "print(np.nanstd(y_with_nan, ddof=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dabaa",
   "metadata": {},
   "source": [
    "Don‚Äôt forget to set the delta degrees of freedom to 1!\n",
    "\n",
    "pd.Series objects also have the method .std() that skips nan by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5defa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.std(ddof=1)\n",
    "\n",
    "z_with_nan.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a522a",
   "metadata": {},
   "source": [
    "The parameter ddof defaults to 1, so you can omit it. Again, if you want to treat nan values differently, then apply the parameter skipna.\n",
    "\n",
    "The **population standard deviation** refers to the entire population. It‚Äôs the positive square root of the population variance. You can calculate it just like the sample standard deviation, with the following differences:\n",
    "\n",
    "- Find the square root of the population variance in the pure Python implementation.\n",
    "- Use statistics.pstdev() instead of statistics.stdev().\n",
    "- Specify the parameter ddof=0 if you use NumPy or pandas. In NumPy, you can omit ddof because its default value is 0.\n",
    "As you can see, you can determine the standard deviation in Python, NumPy, and pandas in almost the same way as you determine the variance. You use different but analogous functions and methods with the same arguments.\n",
    "\n",
    "# Skewness\n",
    "The **sample skewness** measures the asymmetry of a data sample.\n",
    "\n",
    "There are several mathematical definitions of skewness. One common expression to calculate the skewness of the dataset ùë• with ùëõ elements is (ùëõ¬≤ / ((ùëõ ‚àí 1)(ùëõ ‚àí 2))) (Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≥ / (ùëõùë†¬≥)). A simpler expression is Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≥ ùëõ / ((ùëõ ‚àí 1)(ùëõ ‚àí 2)ùë†¬≥), where ùëñ = 1, 2, ‚Ä¶, ùëõ and mean(ùë•) is the sample mean of ùë•. The skewness defined like this is called the **adjusted Fisher-Pearson standardized moment coefficient.**\n",
    "\n",
    "The previous figure showed two datasets that were quite symmetrical. In other words, their points had similar distances from the mean. In contrast, the following image illustrates two asymmetrical sets:\n",
    "\n",
    "The first set is represented by the green dots and the second with the white ones. Usually, **negative skewness** values indicate that there‚Äôs a dominant tail on the left side, which you can see with the first set. **Positive skewness values** correspond to a longer or fatter tail on the right side, which you can see in the second set. If the skewness is close to 0 (for example, between ‚àí0.5 and 0.5), then the dataset is considered quite symmetrical.\n",
    "\n",
    "Once you‚Äôve calculated the size of your dataset n, the sample mean mean_, and the standard deviation std_, you can get the sample skewness with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "196c6571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.947043227390592"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "n = len(x)\n",
    "mean_ = sum(x) / n\n",
    "var_ = sum((item - mean_)**2 for item in x) / (n - 1)\n",
    "std_ = var_ ** 0.5\n",
    "skew_ = (sum((item - mean_)**3 for item in x)\n",
    "        * n / ((n - 1) * (n - 2) * std_**3))\n",
    "skew_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12c9ff",
   "metadata": {},
   "source": [
    "The skewness is positive, so x has a right-side tail.\n",
    "\n",
    "You can also calculate the sample skewness with scipy.stats.skew():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37e116fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9470432273905927\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "y, y_with_nan = np.array(x), np.array(x_with_nan)\n",
    "print(scipy.stats.skew(y, bias=False))\n",
    "print(scipy.stats.skew(y_with_nan, bias=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ae83",
   "metadata": {},
   "source": [
    "The obtained result is the same as the pure Python implementation. The parameter bias is set to False to enable the corrections for statistical bias. The optional parameter nan_policy can take the values 'propagate', 'raise', or 'omit'. It allows you to control how you‚Äôll handle nan values.\n",
    "\n",
    "pandas Series objects have the method .skew() that also returns the skewness of a dataset:\n",
    "\n",
    "Like other methods, .skew() ignores nan values by default, because of the default value of the optional parameter skipna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42eb7507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9470432273905924\n",
      "1.9470432273905924\n"
     ]
    }
   ],
   "source": [
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)\n",
    "print(z.skew())\n",
    "print(z_with_nan.skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086454f",
   "metadata": {},
   "source": [
    "# Percentiles\n",
    "The sample ùëù percentile is the element in the dataset such that ùëù% of the elements in the dataset are less than or equal to that value. Also, (100 ‚àí ùëù)% of the elements are greater than or equal to that value. If there are two such elements in the dataset, then the sample ùëù percentile is their arithmetic mean. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts:\n",
    "\n",
    "- The first quartile is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset.\n",
    "- The second quartile is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles.\n",
    "- The third quartile is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset.\n",
    "Each part has approximately the same number of items. If you want to divide your data into several intervals, then you can use statistics.quantiles():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-5.0, -1.1, 0.1, 2.0, 8.0, 12.8, 21.0, 25.8, 41.0]\n",
    "print(statistics.quantiles(x, n=2))\n",
    "\n",
    "print(statistics.quantiles(x, n=4, method='inclusive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05584c14",
   "metadata": {},
   "source": [
    "In this example, 8.0 is the median of x, while 0.1 and 21.0 are the sample 25th and 75th percentiles, respectively. The parameter n defines the number of resulting equal-probability percentiles, and method determines how to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf2f2f",
   "metadata": {},
   "source": [
    "You can also use np.percentile() to determine any sample percentile in your dataset. For example, this is how you can find the 5th and 95th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74a0c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n",
      "23.999999999999996\n"
     ]
    }
   ],
   "source": [
    "y = np.array(x)\n",
    "print(np.percentile(y, 5))\n",
    "\n",
    "print(np.percentile(y, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3f702",
   "metadata": {},
   "source": [
    "percentile() takes several arguments. You have to provide the dataset as the first argument and the percentile value as the second. The dataset can be in the form of a NumPy array, list, tuple, or similar data structure. The percentile can be a number between 0 and 100 like in the example above, but it can also be a sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22f2abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(y, [25, 50, 75])\n",
    "np.median(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f496a8d",
   "metadata": {},
   "source": [
    "This code calculates the 25th, 50th, and 75th percentiles all at once. If the percentile value is a sequence, then percentile() returns a NumPy array with the results. The first statement returns the array of quartiles. The second statement returns the median, so you can confirm it‚Äôs equal to the 50th percentile, which is 8.0.\n",
    "\n",
    "If you want to ignore nan values, then use np.nanpercentile() instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b2c1993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.   1.   nan  2.5  4.  28. ]\n",
      "[2.5 4.  8. ]\n"
     ]
    }
   ],
   "source": [
    "y_with_nan = np.insert(y, 2, np.nan)\n",
    "print(y_with_nan)\n",
    "print(np.nanpercentile(y_with_nan, [25, 50, 75]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b4352",
   "metadata": {},
   "source": [
    "That‚Äôs how you can avoid nan values.\n",
    "\n",
    "NumPy also offers you very similar functionality in quantile() and nanquantile(). If you use them, then you‚Äôll need to provide the quantile values as the numbers between 0 and 1 instead of percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1becf479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 4. , 8. ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(y, 0.05)\n",
    "np.quantile(y, 0.95)\n",
    "np.quantile(y, [0.25, 0.5, 0.75])\n",
    "np.nanquantile(y_with_nan, [0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904debf",
   "metadata": {},
   "source": [
    "The results are the same as in the previous examples, but here your arguments are between 0 and 1. In other words, you passed 0.05 instead of 5 and 0.95 instead of 95.\n",
    "\n",
    "pd.Series objects have the method .quantile():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b965175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n",
      "23.999999999999996\n",
      "0.25    2.5\n",
      "0.50    4.0\n",
      "0.75    8.0\n",
      "dtype: float64\n",
      "0.25    2.5\n",
      "0.50    4.0\n",
      "0.75    8.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "z, z_with_nan = pd.Series(y), pd.Series(y_with_nan)\n",
    "print(z.quantile(0.05))\n",
    "\n",
    "print(z.quantile(0.95))\n",
    "\n",
    "print(z.quantile([0.25, 0.5, 0.75]))\n",
    "\n",
    "print(z_with_nan.quantile([0.25, 0.5, 0.75]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29b772",
   "metadata": {},
   "source": [
    ".quantile() also needs you to provide the quantile value as the argument. This value can be a number between 0 and 1 or a sequence of numbers. In the first case, .quantile() returns a scalar. In the second case, it returns a new Series holding the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
